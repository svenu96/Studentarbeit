# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iCvbzjubIq-jX9Rj3YPeOHe1LxW2yKJf
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Define the path to your folder containing CSV files
folder_path = '/content/drive/MyDrive/OCDetect_Export'

# Define the number of CSV files to load
num_files_to_load = 10  # Adjust this number to load the desired amount

# Initialize a list to store loaded DataFrames
dataframes = []

# Counter to keep track of loaded files
count = 0

# Loop through the CSV files in the folder and load the specified number of files
for filename in os.listdir(folder_path):
    if filename.endswith('.csv'):
        file_path = os.path.join(folder_path, filename)
        file_dataframes = []

        # Load the entire CSV file without chunking
        df = pd.read_csv(file_path, engine='python')

        #  preprocess the DataFrame here (e.g., dropping columns)
        columns_to_drop = ['datetime', 'timestamp', 'user yes/no', 'compulsive', 'urge', 'tense', 'ignore']
        df = df.drop(columns=columns_to_drop, errors='ignore')
        dataframes.append(df)

        count += 1

        # Check if the desired number of files has been loaded
        if count >= num_files_to_load:
            break

if dataframes:
    for i in range(len(dataframes)):
        print(dataframes[i].columns)
        print(dataframes[i].shape)

for df in dataframes:
    sum_relabeled_2 = (df['relabeled'] == 2).sum()
    print(f"DataFrame : Sum of 'relabeled=2' = {sum_relabeled_2}")
    sum_relabeled_1 = (df['relabeled'] == 1).sum()
    print(f"DataFrame : Sum of 'relabeled=1' = {sum_relabeled_1}")

c = 0
ct = 0
ct1 = 0


for df in dataframes:
    if 'relabeled' in df.columns:
        c += (df['relabeled'] == 2).sum()
        ct += (df['relabeled'] == 1).sum()
        ct1 += (df['relabeled'] == 0).sum()


print(c)
print(ct)
print(ct1)

from imblearn.under_sampling import RandomUnderSampler
# Combine all the loaded DataFrames into one DataFrame
combined_df = pd.concat(dataframes)
print(combined_df.head())
ranund = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = ranund.fit_resample(combined_df.drop(columns=['relabeled']),combined_df['relabeled'])
# Create a new DataFrame with the resampled data and labels
balanced_data = pd.DataFrame(X_resampled, columns=combined_df.drop(columns=['relabeled']).columns)
print(balanced_data.shape)
balanced_data['relabeled'] = y_resampled
print(balanced_data.shape)
# Split data into train and test sets
train_data, test_data = train_test_split(balanced_data, test_size=0.4, random_state=42)
test_data.value_counts()

# Sliding window size and step size
window_size = 150
step_size = 50

# Lists to hold windowed data and labels
trainw = []
trainl = []

data = train_data.values  # Convert DataFrame to numpy array
num_train = len(data)

# Create sliding windows and labels for training
for i in range(0, num_train - window_size + 1, step_size):
    window = data[i:i + window_size, :-1]  # Exclude the last column (label)
    trainw.append(window)
    label_window = data[i:i + window_size, -1]  # Label column in the window
    # Majority voting for label
    label_window = label_window.astype(int)  # Convert label column to integer
    majority_label = np.bincount(label_window).argmax()
    trainl.append(majority_label)
# Convert train_windows and train_labels to numpy arrays
train_window = np.array(trainw)
train_label = np.array(trainl)

# Reshape train_windows to a 2D array for training
num_trainsamp, num_fea_trainsamp = train_window.shape[0], np.prod(train_window.shape[1:])
train_window_reshaped = train_window.reshape(num_trainsamp, num_fea_trainsamp)
#print(train_window_reshaped)

# Initialize and train a RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10, random_state=42)

clf.fit(train_window_reshaped, train_label)

# Cross-validation
cv_scores = cross_val_score(clf, train_window_reshaped, train_label, cv=5)
print("Cross-Validation Scores:", cv_scores)

# Creating test windows and labels
testw = []
testl = []

data1 = test_data.values  # Convert DataFrame to numpy array
num_test = len(data1)

# Create sliding windows and labels for testing
for i in range(0, num_test - window_size + 1, step_size):
    window = data1[i:i + window_size, :-1]  # Exclude the last column (label)
    testw.append(window)
    label_window = data1[i:i + window_size, -1]  # Label column in the window
    # Majority voting for label
    label_window = label_window.astype(int)  # Convert label column to integer
    majority_label = np.bincount(label_window).argmax()
    testl.append(majority_label)

# Convert test windows and labels to numpy arrays
test_windows = np.array(testw)
test_labels = np.array(testl)

# Reshape test_windows to a 2D array for testing
num_testsamp, num_fea_testsamp = test_windows.shape[0], np.prod(test_windows.shape[1:])

test_windows_reshaped = test_windows.reshape(num_testsamp, num_fea_testsamp)

# Predict
prediction = clf.predict(test_windows_reshaped)

# Calculate accuracy and F1 score
accuracy = accuracy_score(test_labels, prediction)
f1 = f1_score(test_labels, prediction, average='micro')
# Create a confusion matrix
cm = confusion_matrix(test_labels, prediction)
print("confusion_matrix:",cm)
print("Accuracy:", accuracy)
print("F1 Score:", f1)
cm = metrics.confusion_matrix(test_labels, prediction)
plt.figure(figsize=(5,5))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.xlabel('Predicted label');
plt.ylabel( 'Actual label');
all_sample_title = 'Accuracy Score: {0}'.format(accuracy)
plt.title(all_sample_title, size = 15);
plt.savefig('random.png')
print(classification_report(test_labels, prediction))